---
date: ' 2020-03-18T17:01:19 '
draft: false
linktitle: Lab 8
menu: 
     MES623Econ:
          parent: Lab Material
          weight: 8
title: NA
type: docs
weight: 8
output:
  blogdown::html_page:
    toc: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Basic Linear Regression

A technique that can be used for prediction, estimation, hypotheses testing, and modeling casual relationships. 

We are typically interested in the relationship between $X$, our independent or explanatory, variable, and $Y$, the dependent or response variable. You can have categorical (e.g. species) and/or numeric (e.g. height measurements) explanatory variables within your linear models. 

The equation for a basic linear regression is $Y=\alpha+\beta X+\varepsilon$, where $\alpha$ is the intercept, $\beta$ is the coefficient or slope, and $\varepsilon$ represents the error structure of the data.

In R, an Ordinary Least Square Regression uses a minimizing routine to estimate the $\alpha$ and $\beta$ parameters. The routine minimizes the sum of squares. The function $\sum (Y_i - \hat{Y_i})^{2}$ represents the sum of squares, where the predicted value $\hat{Y_i}$, for each $X_i$ is calculated by $\hat{Y_i} = \alpha +\beta X$. $Y_i$ is the observed value of $Y$ at $X_i$. **Does this sound like an optimization problem yet?** 

Specifically, $\beta$ represents the relationship between $X$ and $Y$ and is verbally interpreted as "the expected change in the average response variable ($Y$) that is associated with an additional unit of the explanatory variable ($X$)." Sound familiar? This is similar to the **marginal** cost/benefit of purchasing/producing 1 additional unit. If $\beta$ is zero then the regression is not significant and there is no relationship between $X$ and $Y$ (i.e. no change in marginal cost or benefit).

![Possible XY relationships](/Users/lxd312/Box/MES623_Lab/Lab_8/slope_graphs.jpg)

## Assumptions
There are 3 basic assumptions of linear regression:

1) There is a linear relationship in the true population that we are estimating from a sample.

2) The response values, $Y$, are randomly drawn from a normal distribution about the regression line at each $X$ value.

3) There is equal variance in the distribution of observed Y values at all values of $X$. Basically, the residuals are all about the same distance from the regression line. What is a residual? It is the difference between the observed value and the predicted value, $Y_i - \hat{Y_i}$, given the estimated $\alpha$ and $\beta$ parameters.

![Observed vs. predicted Y values showing a linear relationship and constant variance](/Users/lxd312/Box/MES623_Lab/Lab_8/RegressionAssumptions.jpg)

**How do we test these assumptions?**

Look at plots of your data and residuals for abnormalities. Here is a really good [website](http://people.duke.edu/~rnau/testing.htm) for regression diagnostics. 

## R Example
We are going to use fictitious data about voting on climate change legislation. The data includes categorical variables for age and income, as well as numerical variables for willingness to pay (WTP) in taxes and percentage of pollution abatement if the legislation goes into effect.

Your task is to create a linear model that predicts voting 'yes/1' based on age, income, WTP, and pollution percent abatement.

Let's read in our data. 
```{r}
Data = read.csv("ClimateChangeVote.csv")

#Check to make sure it's all there 
head(Data)
```

The linear model function is part of the R base stats package so there is no need to install a new package.

We use the `lm()` function where the "lm" stands for "**l**inear **m**odels." The models have a setup using $\sim$ , where $y\sim x$ means that Y is a function of X. We also have to point R to which dataset to use. You would typically do some data exploration to ensure the relationship is in fact linear before fitting a model, but we are skipping that step today.
```r
lm(y ~ x1 + x2 + x3, Data = Data)
```
Let's run a model with our climate change voting data. We tell R that we have a categorical factor with multiple levels such as age or income by using the function `factor()`.
```{r}
climate.lm = lm(vote ~perAbate +factor(income_lev)+factor(age)+tax, data = Data)
```

To get a summary of the linear model output we use the function `summary()`.
```{r}
summary(climate.lm)
```

Personally, I would 'relevel' my factors in the model so that my intercept includes the effect of the first level of all of my factors. In other words, instead of my intercept being based on "fiftyplus" and "poor/1" (the levels not shown in the summary output), I would change my intercept to be based on the youngest age class which is "twenty." We do this using the `relevel()` function.
```{r}
Data$age <- relevel(Data$age, ref="twenty")
```

**Great, you have some output! But what does all of this mean? Read through the definitions below and then work on some interpretations.**

## Definitions
**Intercept** - Average value of the response with all other factors at their reference level.

**Coefficients** - Slope; how much of an effect each additional unit of the explanatory value has on the response variable.

**Coefficient Standard Error** - How much, on average, the coefficient estimate varies from the actual average value of our response variable.

**Coefficient t-value and p-values** - Used for hypotheses testing -- is there a significant relationship between the response and the coefficient? A p-value of 5% or less shows a significant relationship.

**Residual Standard Error ** - How much the response can deviate from the regression line, on average. It's used along with the F-Statistic to determine the fit of the model to the data and can be compared between models. Different models, fit to the same data can be compared using the F-statistic.

**R-squared** - A measure of the linear relationship between the $X$ and the $Y$ variables. Stays between 0 and 1: 0 - the model does not explain the variance in the response variable; 1 - the model explains the variance in the response variable.

## Interpretation of Results
We are making the assumption that the relationship of the  conditional mean of the probability of saying 'yes' to the variables included, is linear. 

Because we are using both continuous (i.e.,perAbate and tax) and categorical (i.e., age and income) variables, we have to remember that their interpretations are slightly different. 

Continuous variables show the expected marginal increase in the conditional mean of the probability of saying 'yes.' In other words, every additional unit of tax decreases the probability of saying 'yes' by 0.0009748. Every additional unit of percent abatement increases the probability of saying 'yes' by 0.00204.

For the categorical variables, each of the possible levels of that variable increases (or decreases) the expected probability of saying yes given the specific coefficient of that level. 

Note that in our model, only the intercept is statistically different than zero (p-value < 0.05). The coefficients for the categorical values not reported in the table are considered as the reference level/intercept. Our current reference level is fifty plus years old and poor. All coefficients represent changes from that intercept value.

Personally, I would relevel my factors in the model so that my intercept includes the effect of the first level of all of my factors. In other words, instead of my intercept being based on "fiftyplus" and "poor/1" (the levels not shown in the summary output), I would change my intercept to be based on the youngest age class which is "twenty". We do this using the `relevel()` function. **See how this impacts your results.**
```{r}
Data$age <- relevel(Data$age, ref="twenty")
```

Overall, given the R-squared values and the lack of significance of the various explanatory variables, we can conclude that this model is probably not the best model from which to draw conclusions. We would then examine the residuals and the data for violations of assumptions and explore suggestions for alternative models.

## Assumption Violations
Linear regression only works if the response values have a linear relationship with $X$. If the relationship is not linear, we can do the following:

1) Transform the data to make it appear linear for analysis purposes (e.g. log transform).
2) Use a non-linear regression (e.g., a logistic regression).
3) Investigate non-parametric ways of fitting a model to data (e.g., General Additive Models).
4) Consider a quadratic relationship.

Click [here](http://people.duke.edu/~rnau/testing.htm) for more information on how to address assumption violations.

## Additional resources [^1]
Here are some sites that can help with regression:

[Regression example and output explanation](https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R)

[Grant McDermott's GitHub Page](https://raw.githack.com/grantmcdermott/R-intro/master/regression-intro.html)



[^1]: Figures are from Dr. Elizabeth Babcock's Statistics for Environmental Management course notes at the University of Miami RSMAS.
